{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "0273f0ae90ea491ab77d6b51d2f9e4ad",
        "deepnote_cell_type": "text-cell-p"
      },
      "source": "",
      "block_group": "a5ebbfad5ebf438d891569b67370ad2b"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "b37c5e4070274552b2388511d7aa5f15",
        "deepnote_cell_type": "text-cell-p"
      },
      "source": "",
      "block_group": "34f7c9301f2f4335b513a93e163ec8b4"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "337418cc2025434b9f1b0535124b3edb",
        "deepnote_cell_type": "text-cell-p"
      },
      "source": "",
      "block_group": "bf653ea63e7c498faa4d55fc96ac280c"
    },
    {
      "cell_type": "code",
      "metadata": {
        "source_hash": "e2d3e0f2",
        "execution_start": 1744487410100,
        "execution_millis": 1176,
        "execution_context_id": "0f29fcc4-b18b-4921-99f4-7228d1986f4c",
        "cell_id": "d3d734d2ff3b469aa5e3f9a74dc7bf7d",
        "deepnote_cell_type": "code"
      },
      "source": "import numpy as np\nimport pandas as pd\nimport torch\nimport torchvision\nfrom torchvision import transforms\nimport torch.nn as nn\nimport torch.optim as optim\nfrom PIL import Image",
      "block_group": "5abc62bd476f4965905905507973aa16",
      "execution_count": 5,
      "outputs": [],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "source_hash": "c49a4f71",
        "execution_start": 1744489859552,
        "execution_millis": 4582,
        "deepnote_table_state": {
          "sortBy": [],
          "filters": [],
          "pageSize": 10,
          "pageIndex": 40103,
          "columnOrder": [
            "isic_id",
            "target",
            "patient_id",
            "age_approx",
            "sex",
            "anatom_site_general",
            "clin_size_long_diam_mm",
            "image_type",
            "tbp_tile_type",
            "tbp_lv_A",
            "tbp_lv_Aext",
            "tbp_lv_B",
            "tbp_lv_Bext",
            "tbp_lv_C",
            "tbp_lv_Cext",
            "tbp_lv_H",
            "tbp_lv_Hext",
            "tbp_lv_L",
            "tbp_lv_Lext",
            "tbp_lv_areaMM2",
            "tbp_lv_area_perim_ratio",
            "tbp_lv_color_std_mean",
            "tbp_lv_deltaA",
            "tbp_lv_deltaB",
            "tbp_lv_deltaL",
            "tbp_lv_deltaLB",
            "tbp_lv_deltaLBnorm",
            "tbp_lv_eccentricity",
            "tbp_lv_location",
            "tbp_lv_location_simple",
            "tbp_lv_minorAxisMM",
            "tbp_lv_nevi_confidence",
            "tbp_lv_norm_border",
            "tbp_lv_norm_color",
            "tbp_lv_perimeterMM",
            "tbp_lv_radial_color_std_max",
            "tbp_lv_stdL",
            "tbp_lv_stdLExt",
            "tbp_lv_symm_2axis",
            "tbp_lv_symm_2axis_angle",
            "tbp_lv_x",
            "tbp_lv_y",
            "tbp_lv_z",
            "attribution",
            "copyright_license",
            "lesion_id",
            "iddx_full",
            "iddx_1",
            "iddx_2",
            "iddx_3",
            "iddx_4",
            "iddx_5",
            "mel_mitotic_index",
            "mel_thick_mm",
            "tbp_lv_dnn_lesion_confidence"
          ],
          "hiddenColumnIds": [],
          "columnDisplayNames": [],
          "conditionalFilters": [],
          "cellFormattingRules": [],
          "wrappedTextColumnIds": []
        },
        "execution_context_id": "0f29fcc4-b18b-4921-99f4-7228d1986f4c",
        "deepnote_table_loading": false,
        "cell_id": "e195281a14a24be3b5d0fc8668e93628",
        "deepnote_cell_type": "code"
      },
      "source": "# import the data into pandas dataframes\n# 2 dataframes: train and test\ntrain_df = pd.read_csv('/work/20250412-194207/train-metadata.csv')\ntest_df = pd.read_csv('/work/20250412-194207/test-metadata.csv')\n\n# split train into train + val\nperm = np.random.permutation(np.arange(len(train_df)))\n# 80% train, 20% validation\nsplit_idx = int(0.8*len(train_df))\nval_df = train_df.iloc[perm[split_idx:]]\ntrain_df = train_df.iloc[perm[:split_idx]]",
      "block_group": "2fbf058f326c4c95b33505a0b55943dc",
      "execution_count": 65,
      "outputs": [],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "source_hash": "803aad34",
        "execution_start": 1744489023892,
        "execution_millis": 1,
        "execution_context_id": "0f29fcc4-b18b-4921-99f4-7228d1986f4c",
        "cell_id": "7f996b3bc7eb43b1ba0b84077206c52d",
        "deepnote_cell_type": "code"
      },
      "source": "",
      "block_group": "bc5892756e764d89b4dd01e1fbe267d9",
      "execution_count": 46,
      "outputs": [],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "source_hash": "97c8fdff",
        "execution_start": 1744489887497,
        "execution_millis": 0,
        "execution_context_id": "0f29fcc4-b18b-4921-99f4-7228d1986f4c",
        "cell_id": "4be8190c055d42a1a8f4c2cdfad6fee5",
        "deepnote_cell_type": "code"
      },
      "source": "# define transforms for the images\nimage_scale = 128\nchosen_transforms = transforms.Compose([\n    transforms.Resize(image_scale),\n    transforms.ToTensor()\n])\n",
      "block_group": "7d2f24aee09944f29f40e4a5c6c49f45",
      "execution_count": 67,
      "outputs": [],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "source_hash": "4fa52ada",
        "execution_start": 1744489889654,
        "execution_millis": 5,
        "execution_context_id": "0f29fcc4-b18b-4921-99f4-7228d1986f4c",
        "cell_id": "0e3e13b1863f40e4a6a3aba73c2cbb79",
        "deepnote_cell_type": "code"
      },
      "source": "class CustomDataset():\n    def __init__(self, dataframe, transformations):\n        self.df = dataframe\n        self.transformations = transformations\n\n    def __len__(self):\n        # return a length; probably len of dataframe\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        # return (data, label) tuple\n        image_name = self.df.iloc[idx]['isic_id']\n        image_file = '/work/20250412-194207/train-image/image/' + image_name + '.jpg'\n        image_data = Image.open(image_file)\n        image_data = self.transformations(image_data)\n\n        # get the label\n        target = self.df.iloc[idx]['target']\n        \n        # return\n        return image_data, target",
      "block_group": "1aaeb41f99354bfbb7dbeb5c753c2ae0",
      "execution_count": 69,
      "outputs": [],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "source_hash": "f162c75a",
        "execution_start": 1744490991707,
        "execution_millis": 0,
        "execution_context_id": "0f29fcc4-b18b-4921-99f4-7228d1986f4c",
        "cell_id": "4803db3f2b4e4711b8992b95192d067a",
        "deepnote_cell_type": "code"
      },
      "source": "# load the custom datasets\ntrain_dataset = CustomDataset(train_df, chosen_transforms)\nval_dataset = CustomDataset(val_df, chosen_transforms)\ntest_dataset = CustomDataset(test_df, chosen_transforms)",
      "block_group": "e20b0085e3fa425ba81eda82ebb3fa5c",
      "execution_count": 77,
      "outputs": [],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "source_hash": "cedcf728",
        "execution_start": 1744490992210,
        "execution_millis": 2,
        "execution_context_id": "0f29fcc4-b18b-4921-99f4-7228d1986f4c",
        "cell_id": "6d2cdc44ace7416cac9234385e0e4a2d",
        "deepnote_cell_type": "code"
      },
      "source": "# use a dataloader object for efficient memory use\nfrom torch.utils.data import DataLoader\nBATCH_SIZE = 32\nN_WORKERS = 4\n\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, num_workers=N_WORKERS)\nval_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, num_workers=N_WORKERS)\ntest_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, num_workers=N_WORKERS)",
      "block_group": "72648a09a1064585b9137e8be366edf8",
      "execution_count": 79,
      "outputs": [],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "source_hash": "14503a08",
        "execution_start": 1744491293897,
        "execution_millis": 0,
        "execution_context_id": "0f29fcc4-b18b-4921-99f4-7228d1986f4c",
        "cell_id": "615213977ce84ff4bf44c722d314771c",
        "deepnote_cell_type": "code"
      },
      "source": "import torch.nn.functional as F\n\nclass CNN(nn.Module):\n    def __init__(self):\n        # CNN:\n            # 128x128x3 -> 5x5 Conv (padding) + 2x2 Max Pool -> 64x64x8\n            # 64x64x8 -> 5x5 Conv (padding) + 2x2 Max Pool -> 32x32x16\n            # 32x32x16 -> 5x5 Conv (padding) + 2x2 Max Pool -> 16x16x32\n            # 16x16x32 -> 5x5 Conv (padding) + 2x2 Max Pool -> 8x8x64\n            # 8x8x64 -> 5x5 Conv (padding) + 2x2 Max Pool -> 4x4x128\n            # 4x4x128 -> 5x5 Conv (padding) + 2x2 Max Pool -> 2x2x256\n            # 2x2x256 -> 5x5 Conv (padding) + 2x2 Max Pool -> 1x1x512\n            # ReLU on every layer\n            # Flatten to a 512x1 vector\n        # Feedforward:\n            # 512 -> 1024 -> 256 -> 64 -> 16 -> 1\n            # ReLU everywhere, sigmoid estimate at the end\n        super(CNN, self).__init__()\n\n        self.Conv1 = nn.Conv2d(in_channels=3, out_channels=8, kernel_size=5, padding=2)\n        self.Conv2 = nn.Conv2d(in_channels=8, out_channels=16, kernel_size=5, padding=2)\n        self.Conv3 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=5, padding=2)\n        self.Conv4 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=5, padding=2)\n        self.Conv5 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=5, padding=2)\n        self.Conv6 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=5, padding=2)\n        self.Conv7 = nn.Conv2d(in_channels=256, out_channels=512, kernel_size=5, padding=2)\n        \n        self.fc1 = nn.Linear(in_features=512, out_features=1024)\n        self.fc2 = nn.Linear(in_features=1024, out_features=256)\n        self.fc3 = nn.Linear(in_features=256, out_features=64)\n        self.fc4 = nn.Linear(in_features=64, out_features=16)\n        self.fc5 = nn.Linear(in_features=16, out_features=1)\n\n        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n\n    def forward(self, x):\n        image = x\n        # send the image through the Conv layers\n        out = self.pool(F.relu(self.Conv1(image)))\n        out = self.pool(F.relu(self.Conv2(out)))\n        out = self.pool(F.relu(self.Conv3(out)))\n        out = self.pool(F.relu(self.Conv4(out)))\n        out = self.pool(F.relu(self.Conv5(out)))\n        out = self.pool(F.relu(self.Conv6(out)))\n        out = self.pool(F.relu(self.Conv7(out)))\n\n        # flatten\n        out = out.view(-1)\n\n        # send through feedforward layers\n        out = F.relu(self.fc1(out))\n        out = F.relu(self.fc2(out))\n        out = F.relu(self.fc3(out))\n        out = F.relu(self.fc4(out))\n        out = F.sigmoid(self.fc5(out))\n\n        # return prediction\n        return out",
      "block_group": "1cc85393072748f0b193c5eb497aecd7",
      "execution_count": 87,
      "outputs": [],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "source_hash": "2d6aa987",
        "execution_start": 1744491296358,
        "execution_millis": 35,
        "execution_context_id": "0f29fcc4-b18b-4921-99f4-7228d1986f4c",
        "cell_id": "d97a9928c4d54e478ca828f26c657e5e",
        "deepnote_cell_type": "code"
      },
      "source": "# define the model\nmodel = CNN()",
      "block_group": "03e320606e734c77ab31fbc488968b01",
      "execution_count": 89,
      "outputs": [],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "source_hash": "82ec4a58",
        "execution_start": 1744491427657,
        "execution_millis": 0,
        "execution_context_id": "0f29fcc4-b18b-4921-99f4-7228d1986f4c",
        "cell_id": "a1016e48f70c47e1ada61ea4365d6430",
        "deepnote_cell_type": "code"
      },
      "source": "# define criterion and optimizer\ncriterion = nn.BCELoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)",
      "block_group": "ba3def0365d04a3c8aec4059911d583f",
      "execution_count": 95,
      "outputs": [],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "089cddbfb41b4d4c814d16b1b6cbf4d8",
        "deepnote_cell_type": "code"
      },
      "source": "import time\n\n# just 1 epoch for training because we have ~320000 images in the training dataset\nN_EPOCHS = 1\n# how often should we print?\n# 320000/32 (batch) = 10000, so 10 prints is a frequency of 1000\nFREQ = 1000\n\n# train the model!\nfor epoch in range(N_EPOCHS):\n    running_loss = 0.0\n    start_time = time.time()\n\n    for idx, data in enumerate(train_loader):\n        # get the data\n        inputs, labels = data\n\n        # zero the gradient\n        optimizer.zero_grad()\n        \n        # forward\n        y_pred == model(inputs)\n        # loss\n        # labels are ints but our predictions are floats\n        loss = criterion(y_pred, labels.float())\n        # backpropagation\n        loss.backward()\n        optimizer.step()\n\n        # add loss, print at FREQ\n        running_loss += loss.item()\n        if idx+1 % FREQ == 0:\n            print(f\"elapsed time: {time.time()-start_time:.3f}, average loss: {running_loss/FREQ}\")\n            start_time = time.time()\n            running_loss=0.0",
      "block_group": "f4daac46abe144c0ab5d20d3109b5293",
      "execution_count": null,
      "outputs": [],
      "outputs_reference": null,
      "content_dependencies": null
    },
    {
      "cell_type": "markdown",
      "source": "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=79063c31-68e7-42ba-ac09-1fa3948bed4b' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,PD94bWwgdmVyc2lvbj0iMS4wIiBlbmNvZGluZz0iVVRGLTgiPz4KPHN2ZyB3aWR0aD0iODBweCIgaGVpZ2h0PSI4MHB4IiB2aWV3Qm94PSIwIDAgODAgODAiIHZlcnNpb249IjEuMSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIiB4bWxuczp4bGluaz0iaHR0cDovL3d3dy53My5vcmcvMTk5OS94bGluayI+CiAgICA8IS0tIEdlbmVyYXRvcjogU2tldGNoIDU0LjEgKDc2NDkwKSAtIGh0dHBzOi8vc2tldGNoYXBwLmNvbSAtLT4KICAgIDx0aXRsZT5Hcm91cCAzPC90aXRsZT4KICAgIDxkZXNjPkNyZWF0ZWQgd2l0aCBTa2V0Y2guPC9kZXNjPgogICAgPGcgaWQ9IkxhbmRpbmciIHN0cm9rZT0ibm9uZSIgc3Ryb2tlLXdpZHRoPSIxIiBmaWxsPSJub25lIiBmaWxsLXJ1bGU9ImV2ZW5vZGQiPgogICAgICAgIDxnIGlkPSJBcnRib2FyZCIgdHJhbnNmb3JtPSJ0cmFuc2xhdGUoLTEyMzUuMDAwMDAwLCAtNzkuMDAwMDAwKSI+CiAgICAgICAgICAgIDxnIGlkPSJHcm91cC0zIiB0cmFuc2Zvcm09InRyYW5zbGF0ZSgxMjM1LjAwMDAwMCwgNzkuMDAwMDAwKSI+CiAgICAgICAgICAgICAgICA8cG9seWdvbiBpZD0iUGF0aC0yMCIgZmlsbD0iIzAyNjVCNCIgcG9pbnRzPSIyLjM3NjIzNzYyIDgwIDM4LjA0NzY2NjcgODAgNTcuODIxNzgyMiA3My44MDU3NTkyIDU3LjgyMTc4MjIgMzIuNzU5MjczOSAzOS4xNDAyMjc4IDMxLjY4MzE2ODMiPjwvcG9seWdvbj4KICAgICAgICAgICAgICAgIDxwYXRoIGQ9Ik0zNS4wMDc3MTgsODAgQzQyLjkwNjIwMDcsNzYuNDU0OTM1OCA0Ny41NjQ5MTY3LDcxLjU0MjI2NzEgNDguOTgzODY2LDY1LjI2MTk5MzkgQzUxLjExMjI4OTksNTUuODQxNTg0MiA0MS42NzcxNzk1LDQ5LjIxMjIyODQgMjUuNjIzOTg0Niw0OS4yMTIyMjg0IEMyNS40ODQ5Mjg5LDQ5LjEyNjg0NDggMjkuODI2MTI5Niw0My4yODM4MjQ4IDM4LjY0NzU4NjksMzEuNjgzMTY4MyBMNzIuODcxMjg3MSwzMi41NTQ0MjUgTDY1LjI4MDk3Myw2Ny42NzYzNDIxIEw1MS4xMTIyODk5LDc3LjM3NjE0NCBMMzUuMDA3NzE4LDgwIFoiIGlkPSJQYXRoLTIyIiBmaWxsPSIjMDAyODY4Ij48L3BhdGg+CiAgICAgICAgICAgICAgICA8cGF0aCBkPSJNMCwzNy43MzA0NDA1IEwyNy4xMTQ1MzcsMC4yNTcxMTE0MzYgQzYyLjM3MTUxMjMsLTEuOTkwNzE3MDEgODAsMTAuNTAwMzkyNyA4MCwzNy43MzA0NDA1IEM4MCw2NC45NjA0ODgyIDY0Ljc3NjUwMzgsNzkuMDUwMzQxNCAzNC4zMjk1MTEzLDgwIEM0Ny4wNTUzNDg5LDc3LjU2NzA4MDggNTMuNDE4MjY3Nyw3MC4zMTM2MTAzIDUzLjQxODI2NzcsNTguMjM5NTg4NSBDNTMuNDE4MjY3Nyw0MC4xMjg1NTU3IDM2LjMwMzk1NDQsMzcuNzMwNDQwNSAyNS4yMjc0MTcsMzcuNzMwNDQwNSBDMTcuODQzMDU4NiwzNy43MzA0NDA1IDkuNDMzOTE5NjYsMzcuNzMwNDQwNSAwLDM3LjczMDQ0MDUgWiIgaWQ9IlBhdGgtMTkiIGZpbGw9IiMzNzkzRUYiPjwvcGF0aD4KICAgICAgICAgICAgPC9nPgogICAgICAgIDwvZz4KICAgIDwvZz4KPC9zdmc+' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>",
      "metadata": {
        "created_in_deepnote_cell": true,
        "deepnote_cell_type": "markdown"
      }
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "deepnote_notebook_id": "514f51dd8eae40bbaedd2cc4ed179662"
  }
}